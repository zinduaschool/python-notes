{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004a55bc",
   "metadata": {},
   "source": [
    "## Introduction to Beautiful Soup\n",
    "\n",
    "**Beautiful Soup** is a Python library used to parse HTML and XML documents. It provides simple methods to **search**, **navigate**, and **modify** the parse tree. Beautiful Soup is designed to be easy and beginner-friendly, making it an excellent tool for web scraping projects.\n",
    "\n",
    "With Beautiful Soup, you can extract data from websites to:\n",
    "- Create reports\n",
    "- Visualize data\n",
    "- Perform analysis\n",
    "\n",
    "### What is Data Parsing?\n",
    "\n",
    "**Data parsing** refers to the process of converting raw data (such as HTML) into a different format that's easier to work with.\n",
    "\n",
    "#### What Does a Data Parser Do?\n",
    "\n",
    "A **data parser**:\n",
    "1. Receives data in a certain format (e.g., HTML).\n",
    "2. Reads the data and stores it as a string.\n",
    "3. Extracts relevant information from the string.\n",
    "4. Optionally cleans or processes the data.\n",
    "5. Outputs it in formats like JSON, CSV, YAML, or stores it in databases.\n",
    "\n",
    "### Example: Why Parsing is Useful\n",
    "\n",
    "**Imagine you're building a price comparison tool**:\n",
    "- It scrapes data from multiple e-commerce sites.\n",
    "- Collects and compares product prices in real time.\n",
    "- Helps users find the best deals.\n",
    "- Increases your traffic and affiliate sales.\n",
    "\n",
    "### Guide to Web Scraping Using Beautiful Soup\n",
    "\n",
    "#### Step 1: Install Required Libraries\n",
    "\n",
    "Install using `pip`:\n",
    "```bash\n",
    "pip install bs4\n",
    "pip install requests\n",
    "```\n",
    "\n",
    "#### Step 2: Import Libraries\n",
    "```python \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "#### Step 3: Send an HTTP Request\n",
    "```python \n",
    "url = 'http://example.com'\n",
    "response = requests.get(url)\n",
    "```\n",
    "\n",
    "#### Step 4: Parse the HTML Content\n",
    "```python\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "```\n",
    "\n",
    "#### Step 5: Extract Data\n",
    "- Example: Extract all ```<h1>``` tags\n",
    "```python\n",
    "h1_tags = soup.find_all('h1')\n",
    "for h1 in h1_tags:\n",
    "    print(h1.text)\n",
    "```\n",
    "\n",
    "#### Step 6: More Advanced Data Extraction\n",
    "- Find by Tag Name\n",
    "\n",
    "```python\n",
    "title = soup.title\n",
    "print(title.text)\n",
    "```\n",
    "\n",
    "- Find by Class Name\n",
    "\n",
    "```python\n",
    "articles = soup.find_all('div', class_='article')\n",
    "for article in articles:\n",
    "    print(article.text)\n",
    "```\n",
    "\n",
    "- Find by ID \n",
    "```python\n",
    "main_content = soup.find(id='main-content')\n",
    "print(main_content.text)\n",
    "```\n",
    "\n",
    "- Extract Attributes\n",
    "```python\n",
    "img_tags = soup.find_all('img')\n",
    "for img in img_tags:\n",
    "    print(img['src'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeb6cd2",
   "metadata": {},
   "source": [
    "### Example Project: Scraping an Online Bookstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991f14f",
   "metadata": {},
   "source": [
    "Ensure that you have both beautifulsoup and requests installed\n",
    "- `pip install bs4`\n",
    "- `pip install requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d72745",
   "metadata": {},
   "source": [
    "#### Step 1: Import Libraries\n",
    "First, we import the tools we’ll use.\n",
    "- `requests`: helps us send a request to the website and get its HTML content.\n",
    "- `BeautifulSoup`: helps us parse (analyze and extract) data from the HTML.\n",
    "- `csv`: This library is used to write data to a CSV (Comma-Separated Values) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06b18765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   #Fetch content from the web.\n",
    "from bs4 import BeautifulSoup # Parse HTML content.\n",
    "import csv # Save data to a CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c6b11",
   "metadata": {},
   "source": [
    "#### Step 2: Send HTTP Request\n",
    "- The `get()` method sends a request to the website.\n",
    "- The website’s server responds with the HTML page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "783874f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://books.toscrape.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c160d078",
   "metadata": {},
   "source": [
    "We can check the status code to make sure the request succeeded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3c2e532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7df43f",
   "metadata": {},
   "source": [
    "#### Step 3: View the Page Content\n",
    "\n",
    "Let’s look at the HTML returned by the site:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29e7ab14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\n",
      "<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\n",
      "<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\n",
      "<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\n",
      "    <head>\n",
      "        <title>\n",
      "    All products | Books to Scrape - Sandbox\n",
      "</title>\n",
      "\n",
      "        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" /\n"
     ]
    }
   ],
   "source": [
    "print(response.text[:500])  # Print the first 500 characters of the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378406c",
   "metadata": {},
   "source": [
    "That’s the raw page content we’ll now parse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837050d4",
   "metadata": {},
   "source": [
    "#### Step 4: Parse the HTML with BeautifulSoup\n",
    "\n",
    "This turns the messy HTML into a structured object (soup) that we can easily search through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00a15b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28098e",
   "metadata": {},
   "source": [
    "| Part                 | Meaning                                                                                                                                                                        |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `response.content`   | This is the **raw HTML content** (in bytes) returned from the website by the `requests.get()` call. It's what the browser would see when loading the page.                     |\n",
    "| `'html.parser'`      | This is the **parser** used by Beautiful Soup to process the HTML. It tells Beautiful Soup how to interpret the structure of the HTML document.                                |\n",
    "| `BeautifulSoup(...)` | This creates a **BeautifulSoup object**, which represents the entire HTML document. This object provides powerful methods to extract elements like `<div>`, `<p>`, `<a>`, etc. |\n",
    "| `soup = ...`         | Stores the parsed result in a variable named `soup`. This is your main entry point to navigate and extract information from the webpage.                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9763e9c",
   "metadata": {},
   "source": [
    "Let’s find all the book titles.\n",
    "When you inspect the site’s HTML, each book title is inside an `<h3>` tag, like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1eb32",
   "metadata": {},
   "source": [
    "So, we can extract them using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da235c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all(\"h3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954b5a4",
   "metadata": {},
   "source": [
    "#### Step 5: Extract the Text from Each Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "592e3e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Light in the Attic\n",
      "Tipping the Velvet\n",
      "Soumission\n",
      "Sharp Objects\n",
      "Sapiens: A Brief History of Humankind\n",
      "The Requiem Red\n",
      "The Dirty Little Secrets of Getting Your Dream Job\n",
      "The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull\n",
      "The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics\n",
      "The Black Maria\n",
      "Starving Hearts (Triangular Trade Trilogy, #1)\n",
      "Shakespeare's Sonnets\n",
      "Set Me Free\n",
      "Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\n",
      "Rip it Up and Start Again\n",
      "Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991\n",
      "Olio\n",
      "Mesaerion: The Best Science Fiction Stories 1800-1849\n",
      "Libertarianism for Beginners\n",
      "It's Only the Himalayas\n"
     ]
    }
   ],
   "source": [
    "for book in titles:\n",
    "    title = book.a['title']\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af231c",
   "metadata": {},
   "source": [
    "#### Step 6: Save the Data to a CSV File\n",
    "Now we can store the titles for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c29b6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"book_titles.csv\", \"w\", newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Book Title\"])  # Writes the header row of the CSV file \n",
    "    for book in titles:\n",
    "        title = book.a['title']\n",
    "        writer.writerow([title])  # Write each title as a new row in the CSV file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6256ad",
   "metadata": {},
   "source": [
    "####  Find Book Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d5cf16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find product containers using find_all()\n",
    "books = soup.find_all('article', class_='product_pod')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f140fc1",
   "metadata": {},
   "source": [
    "- `soup.find_all()` searches through the parsed HTML for all article tags with the class `'product_pod'`.\n",
    "- Each article tag represents a book on the website.\n",
    "- The resulting list of book elements is stored in the books variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1033da",
   "metadata": {},
   "source": [
    "#### Extract Book Data\n",
    "\n",
    "- An empty list `data` is initialized to store the extracted information for each book.\n",
    "- The loop iterates over each book element found in the previous step.\n",
    "    - `title`: Extracts the book’s title by accessing the title attribute of the a tag nested within the `h3` tag of the book element.\n",
    "    - `price`: Extracts the price of the book by finding the `p` tag with the class `'price_color'` and accessing its text content.\n",
    "    - `availability`: Extracts the availability status of the book by finding the `p` tag with the class `'instock availability'` and accessing its text content after stripping any extra whitespace.\n",
    "- The title, price, and availability of each book are stored as a list [title, price, availability], which is then appended to the data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bc1b81a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['A Light in the Attic', '£51.77', 'In stock'],\n",
       " ['Tipping the Velvet', '£53.74', 'In stock'],\n",
       " ['Soumission', '£50.10', 'In stock'],\n",
       " ['Sharp Objects', '£47.82', 'In stock'],\n",
       " ['Sapiens: A Brief History of Humankind', '£54.23', 'In stock'],\n",
       " ['The Requiem Red', '£22.65', 'In stock'],\n",
       " ['The Dirty Little Secrets of Getting Your Dream Job', '£33.34', 'In stock'],\n",
       " ['The Coming Woman: A Novel Based on the Life of the Infamous Feminist, Victoria Woodhull',\n",
       "  '£17.93',\n",
       "  'In stock'],\n",
       " ['The Boys in the Boat: Nine Americans and Their Epic Quest for Gold at the 1936 Berlin Olympics',\n",
       "  '£22.60',\n",
       "  'In stock'],\n",
       " ['The Black Maria', '£52.15', 'In stock'],\n",
       " ['Starving Hearts (Triangular Trade Trilogy, #1)', '£13.99', 'In stock'],\n",
       " [\"Shakespeare's Sonnets\", '£20.66', 'In stock'],\n",
       " ['Set Me Free', '£17.46', 'In stock'],\n",
       " [\"Scott Pilgrim's Precious Little Life (Scott Pilgrim #1)\",\n",
       "  '£52.29',\n",
       "  'In stock'],\n",
       " ['Rip it Up and Start Again', '£35.02', 'In stock'],\n",
       " ['Our Band Could Be Your Life: Scenes from the American Indie Underground, 1981-1991',\n",
       "  '£57.25',\n",
       "  'In stock'],\n",
       " ['Olio', '£23.88', 'In stock'],\n",
       " ['Mesaerion: The Best Science Fiction Stories 1800-1849',\n",
       "  '£37.59',\n",
       "  'In stock'],\n",
       " ['Libertarianism for Beginners', '£51.33', 'In stock'],\n",
       " [\"It's Only the Himalayas\", '£45.17', 'In stock']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for book in books:\n",
    "    title = book.h3.a['title']\n",
    "    price = book.find('p', class_='price_color').text\n",
    "    availability = book.find('p', class_=\"instock availability\").text.strip()\n",
    "    data.append([title, price, availability])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbaa1b",
   "metadata": {},
   "source": [
    "#### Step 6: Write Data to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36114fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to bookstore.csv\n"
     ]
    }
   ],
   "source": [
    "with open('bookstore.csv', 'w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Title\", \"Price\", \"Availability\"])\n",
    "    writer.writerows(data)\n",
    "print(\"Data has been written to bookstore.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae527616",
   "metadata": {},
   "source": [
    "#### Advanced Extraction Techniques\n",
    "\n",
    "- ```soup.find_all('div', class_='some_class')```\n",
    "- Extracting links, images, and nested elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5eaf593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\n",
      "media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg\n",
      "media/cache/3e/ef/3eef99c9d9adef34639f510662022830.jpg\n",
      "media/cache/32/51/3251cf3a3412f53f339e42cac2134093.jpg\n",
      "media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c12a6.jpg\n",
      "media/cache/68/33/68339b4c9bc034267e1da611ab3b34f8.jpg\n",
      "media/cache/92/27/92274a95b7c251fea59a2b8a78275ab4.jpg\n",
      "media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78cc64.jpg\n",
      "media/cache/66/88/66883b91f6804b2323c8369331cb7dd1.jpg\n",
      "media/cache/58/46/5846057e28022268153beff6d352b06c.jpg\n",
      "media/cache/be/f4/bef44da28c98f905a3ebec0b87be8530.jpg\n",
      "media/cache/10/48/1048f63d3b5061cd2f424d20b3f9b666.jpg\n",
      "media/cache/5b/88/5b88c52633f53cacf162c15f4f823153.jpg\n",
      "media/cache/94/b1/94b1b8b244bce9677c2f29ccc890d4d2.jpg\n",
      "media/cache/81/c4/81c4a973364e17d01f217e1188253d5e.jpg\n",
      "media/cache/54/60/54607fe8945897cdcced0044103b10b6.jpg\n",
      "media/cache/55/33/553310a7162dfbc2c6d19a84da0df9e1.jpg\n",
      "media/cache/09/a3/09a3aef48557576e1a85ba7efea8ecb7.jpg\n",
      "media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736406e.jpg\n",
      "media/cache/27/a5/27a53d0bb95bdd88288eaf66c9230d7e.jpg\n"
     ]
    }
   ],
   "source": [
    "images = soup.find_all('img')\n",
    "for img in images:\n",
    "    print(img['src'])  # Print the source URL of each image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535845f5",
   "metadata": {},
   "source": [
    "### Scrape Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2182d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP Request\n",
    "url = \"http://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b047bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes to Scrape\n"
     ]
    }
   ],
   "source": [
    "title = soup.title.text\n",
    "print(title)  # Print the title of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "295840e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find All Quotes\n",
    "# Each quote is inside a <div class=\"quote\"> block.\n",
    "quotes = soup.find_all('div', class_ = 'quote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0be26f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting quotes, authors, and tags\n",
    "quotes_data = []\n",
    "for quote in quotes:\n",
    "    text = quote.find('span', class_='text').text\n",
    "    author = quote.find('small', class_='author').text\n",
    "    tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "    quotes_data.append({\n",
    "        'text': text,\n",
    "        'author': author,\n",
    "        'tags': tags\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1f9d5da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quotes have been written to quotes.csv\n"
     ]
    }
   ],
   "source": [
    "# save the data to a CSV file\n",
    "with open('quotes.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Text', 'Author', 'Tags'])\n",
    "\n",
    "    for quote in quotes_data:\n",
    "        writer.writerow([quote['text'], quote['author'], ', '.join(quote['tags'])])\n",
    "print(\"Quotes have been written to quotes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92ab152",
   "metadata": {},
   "source": [
    "```python\n",
    "<li class=\"next\">\n",
    "  <a href=\"/page/2/\">Next →</a>\n",
    "</li>\n",
    "```\n",
    "- The href=\"/page/2/\" is a relative URL, not the full link.\n",
    "- It only gives the part after the domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d4837",
   "metadata": {},
   "source": [
    "#### Challenge:  Scraping multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff2ddb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: http://quotes.toscrape.com/\n",
      "Scraping page: http://quotes.toscrape.com/page/2/\n",
      "Scraping page: http://quotes.toscrape.com/page/3/\n",
      "Scraping page: http://quotes.toscrape.com/page/4/\n",
      "Scraping page: http://quotes.toscrape.com/page/5/\n",
      "Scraping page: http://quotes.toscrape.com/page/6/\n",
      "Scraping page: http://quotes.toscrape.com/page/7/\n",
      "Scraping page: http://quotes.toscrape.com/page/8/\n",
      "Scraping page: http://quotes.toscrape.com/page/9/\n",
      "Scraping page: http://quotes.toscrape.com/page/10/\n",
      "All quotes scraped and saved to quotes_all_pages.csv.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "base_url = \"http://quotes.toscrape.com\"  # constant part of the site\n",
    "url = \"/\"  # changing part like /, /page/2/\n",
    "\n",
    "quotes_data = []\n",
    "\n",
    "while url:\n",
    "    full_url = base_url + url\n",
    "    print(f\"Scraping page: {full_url}\")  # Message for each page\n",
    "\n",
    "    # Send HTTP request to current page\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract quotes, authors, tags\n",
    "    quotes = soup.find_all('div', class_='quote')\n",
    "    for quote in quotes:\n",
    "        text = quote.find('span', class_='text').text\n",
    "        author = quote.find('small', class_='author').text\n",
    "        tags = [tag.text for tag in quote.find_all('a', class_='tag')]\n",
    "        quotes_data.append({\n",
    "            'text': text,\n",
    "            'author': author,\n",
    "            'tags': tags\n",
    "        })\n",
    "\n",
    "    # Check for the next page\n",
    "    next_btn = soup.find('li', class_='next')\n",
    "    if next_btn:\n",
    "        url = next_btn.a['href']\n",
    "    else:\n",
    "        url = None\n",
    "\n",
    "# Write to CSV\n",
    "with open('quotes_all_pages.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Text', 'Author', 'Tags'])\n",
    "    for quote in quotes_data:\n",
    "        writer.writerow([quote['text'], quote['author'], ', '.join(quote['tags'])])\n",
    "\n",
    "print(\"All quotes scraped and saved to quotes_all_pages.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
